@article{adamsUserFriendlyAutomaticTranscription2021,
  title = {User-{{Friendly Automatic Transcription}} of {{Low-Resource Languages}}: {{Plugging ESPnet}} into {{Elpis}}},
  shorttitle = {User-{{Friendly Automatic Transcription}} of {{Low-Resource Languages}}},
  author = {Adams, Oliver},
  date = {2021},
  journaltitle = {Proceedings of the Workshop on Computational Methods for Endangered Languages},
  shortjournal = {computel},
  volume = {1},
  number = {2},
  doi = {10.33011/computel.v1i.969},
  url = {https://journals.colorado.edu/index.php/computel/article/view/969},
  urldate = {2024-05-29},
  abstract = {This paper reports on progress integrating the speech recognition toolkit ESPnet into Elpis, a web front-end originally designed to provide access to the Kaldi automatic speech recognition toolkit. The goal of this work is to make end-to-end speech recognition models available to language workers via a user-friendly graphical interface. Encouraging results are reported on (i) development of an ESPnet recipe for use in Elpis, with preliminary results on data sets previously used for training acoustic models with the Persephone toolkit along with a new data set that had not previously been used in speech recognition, and (ii) incorporating ESPnet into Elpis along with UI enhancements and a CUDA-supported Dockerfile.},
  langid = {english},
  file = {C\:\\Users\\Yu\\Zotero\\storage\\BN757VAL\\Adams - 2021 - User-Friendly Automatic Transcription of Low-Resou.pdf;C\:\\Users\\Yu\\Zotero\\storage\\UTR7745W\\Adams - 2021 - User-Friendly Automatic Transcription of Low-Resou.pdf}
}

@book{ainsworthMechanismsSpeechRecognition1976,
  title = {Mechanisms of Speech Recognition},
  author = {Ainsworth, William A.},
  date = {1976},
  series = {International Series in Natural Philosophy},
  edition = {1. ed},
  number = {85},
  publisher = {Pergamon Press},
  location = {Oxford},
  isbn = {978-0-08-020394-2 978-0-08-020395-9},
  langid = {english},
  pagetotal = {139}
}

@article{alqadasiImprovingAutomaticForced2024,
  title = {Improving {{Automatic Forced Alignment}} for {{Phoneme Segmentation}} in {{Quranic Recitation}}},
  author = {Alqadasi, Ammar Mohammed Ali and Zeki, Akram M. and Sunar, Mohd Shahrizal and Salam, Md. Sah Bin Hj and Abdulghafor, Rawad and Khaled, Nashwan Abdo},
  date = {2024},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {12},
  pages = {229--244},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3345843},
  url = {https://ieeexplore.ieee.org/document/10371319/},
  urldate = {2024-06-04},
  abstract = {Segmentation plays a crucial role in speech processing applications, where high accuracy is essential. The quest for improved accuracy in automatic segmentation, particularly in the context of the Arabic language, has garnered substantial attention. However, the differences between Qur’an recitation and normal Arabic speech, especially with regard to intonation rules affecting the lengthening of long vowels, pose challenges in segmentation especially for Qur’an recitation. This research endeavors to address these challenges by delving into the domain of automatic segmentation for Qur’an recitation recognition. The proposed scheme employs a hidden Markov models (HMMs) forced alignment algorithm. To enhance the precision of segmentation, several refinements have been introduced, with a primary emphasis on the phonetic model of the Qur’an and Tajweed, particularly the intricate rules governing elongation. These enhancements encompass the adaptation of an acoustic model tailored for Qur’anic recitation as preprocessing and culminate in the development of an algorithm aimed at refining forced alignment based on the phonetic nuances of the Qur’an. These enhancements are seamlessly integrated as post-processing components for the classic HMM-based forced alignment. The research utilizes a comprehensive database featuring recordings from 100 renowned Qur’an reciters, encompassing the recitation of 21 Qur’anic verses (Ayat). Additionally, 30 reciters were asked to record the same verses, incorporating various recitation speed patterns. To facilitate the evaluation process, a Random sample of the Qur’anic database was manually segmented, comprised 21 Ayats, totaling 19,800 words, with 89 unique words (14 verses x 3 recitation levels: fast, slow and normal x 6 readers). The outcomes of this study manifest notable advancements in the alignment of long vowels within Qur’an recitation, all while maintaining the precise alignment of vowels and consonants. Objective comparisons between the proposed automatic methods and manual segmentation were conducted to ascertain the superior approach. The findings affirm that the classic forced alignment method produces satisfactory outcomes when employed on verses lacking long vowels. However, its performance diminishes when confronted with verses containing long vowels. Therefore, the test samples were categorized into three groups based on the presence of long vowels, resulting in a Correct Classification Rate (CCR) that ranged from 6\% to 57\%, contingent on whether the verse includes long vowels or not. The average CCR across all test samples was 23\%. In contrast, the proposed algorithm significantly enhances audio segmentation. It achieved CCR values ranging from 16\% to 70\% within the same database categories, with an average CCR of 45\% across all test samples. This marks a notable advancement of 22\% in segmented speech accuracy, particularly within a 30 ms tolerance, for verses containing long vowels.},
  langid = {english},
  file = {C:\Users\Yu\Zotero\storage\FNAVFHKG\Alqadasi et al. - 2024 - Improving Automatic Forced Alignment for Phoneme S.pdf}
}

@book{arumugamPythonZiRanYuYanChuLiShiZhanHandsonNatural2020,
  title = {Python自然语言处理实战 [{{Hands-on}} Natural Language Processing with {{Python}}]},
  author = {Arumugam, Rajesh and Shanmugamani, Rajalingappaa},
  translator = {Yang, Hang},
  date = {2020},
  publisher = {Posts \& Telecom Press},
  location = {Beijing},
  abstract = {本书介绍自然语言处理和深度学习的核心概念,例如CNN,RNN,语义嵌入和Word2vec等.读者将学习如何使用神经网络执行自然语言处理任务,以及如何在自然语言处理应用程序中训练和部署神经网络等},
  isbn = {978-7-115-54926-6},
  annotation = {OCLC: 1282776239},
  file = {C:\Users\Yu\Zotero\storage\F6SU4J2A\Arumugam and Shanmugamani - 2020 - Python自然语言处理实战 [Hands-on natural language processi.pdf}
}

@article{benzeghibaAutomaticSpeechRecognition2007,
  title = {Automatic Speech Recognition and Speech Variability: {{A}} Review},
  shorttitle = {Automatic Speech Recognition and Speech Variability},
  author = {Benzeghiba, M. and De Mori, R. and Deroo, O. and Dupont, S. and Erbes, T. and Jouvet, D. and Fissore, L. and Laface, P. and Mertins, A. and Ris, C. and Rose, R. and Tyagi, V. and Wellekens, C.},
  date = {2007-10-01},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  series = {Intrinsic {{Speech Variations}}},
  volume = {49},
  number = {10},
  pages = {763--786},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2007.02.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0167639307000404},
  urldate = {2024-07-07},
  abstract = {Major progress is being recorded regularly on both the technology and exploitation of automatic speech recognition (ASR) and spoken language systems. However, there are still technological barriers to flexible solutions and user satisfaction under some circumstances. This is related to several factors, such as the sensitivity to the environment (background noise), or the weak representation of grammatical and semantic knowledge. Current research is also emphasizing deficiencies in dealing with variation naturally present in speech. For instance, the lack of robustness to foreign accents precludes the use by specific populations. Also, some applications, like directory assistance, particularly stress the core recognition technology due to the very high active vocabulary (application perplexity). There are actually many factors affecting the speech realization: regional, sociolinguistic, or related to the environment or the speaker herself. These create a wide range of variations that may not be modeled correctly (speaker, gender, speaking rate, vocal effort, regional accent, speaking style, non-stationarity, etc.), especially when resources for system training are scarce. This paper outlines current advances related to these topics.},
  keywords = {Speech analysis,Speech intrinsic variations,Speech modeling,Speech recognition},
  file = {C\:\\Users\\Yu\\Zotero\\storage\\L3QYWYUS\\Benzeghiba 等 - 2007 - Automatic speech recognition and speech variabilit.pdf;C\:\\Users\\Yu\\Zotero\\storage\\K8R9EJ2P\\S0167639307000404.html}
}

@article{besacierAutomaticSpeechRecognition2014,
  title = {Automatic Speech Recognition for Under-Resourced Languages: {{A}} Survey},
  shorttitle = {Automatic Speech Recognition for Under-Resourced Languages},
  author = {Besacier, Laurent and Barnard, Etienne and Karpov, Alexey and Schultz, Tanja},
  date = {2014-01-01},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {56},
  pages = {85--100},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2013.07.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0167639313000988},
  urldate = {2024-06-04},
  abstract = {Speech processing for under-resourced languages is an active field of research, which has experienced significant progress during the past decade. We propose, in this paper, a survey that focuses on automatic speech recognition (ASR) for these languages. The definition of under-resourced languages and the challenges associated to them are first defined. The main part of the paper is a literature review of the recent (last 8years) contributions made in ASR for under-resourced languages. Examples of past projects and future trends when dealing with under-resourced languages are also presented. We believe that this paper will be a good starting point for anyone interested to initiate research in (or operational development of) ASR for one or several under-resourced languages. It should be clear, however, that many of the issues and approaches presented here, apply to speech technology in general (text-to-speech synthesis for instance).},
  keywords = {Automatic pronunciation generation,Automatic speech recognition (ASR),Crosslingual acoustic modeling and adaptation,Language portability,Lexical modeling,Speech and language resources acquisition,Statistical language modeling,Under-resourced languages},
  file = {C\:\\Users\\Yu\\Zotero\\storage\\3DXR3MWZ\\Besacier et al. - 2014 - Automatic speech recognition for under-resourced l.pdf;C\:\\Users\\Yu\\Zotero\\storage\\Y8VCQMWU\\S0167639313000988.html}
}

@software{boersmaPraatSystemDoing2023,
  title = {Praat, a System for Doing Phonetics by Computer.},
  author = {Boersma, Paul and Weenink, David},
  date = {2023},
  url = {https://www.fon.hum.uva.nl/praat/},
  urldate = {2022-06-07},
  version = {Version 6.3.09},
  file = {C:\Users\Yu\Zotero\storage\P2P8PC25\praat.html}
}

@article{chenMetaAdversarialLearning2024,
  title = {Meta Adversarial Learning Improves Low-Resource Speech Recognition},
  author = {Chen, Yaqi and Yang, Xukui and Zhang, Hao and Zhang, Wenlin and Qu, Dan and Chen, Cong},
  date = {2024-03-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {84},
  pages = {101576},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2023.101576},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230823000955},
  urldate = {2024-06-04},
  abstract = {Low-resource automatic speech recognition is a challenging task. To resolve this issue, multilingual meta-learning learns a better model initialization from many source languages, allowing for rapid adaption to target languages. However, differences in data scales and learning difficulties vary greatly from one language to another. As a result, the model favors large-scale and simple source languages. Moreover, the shared semantic space of various languages is difficult to learn due to a lack of restrictions on multilingual pre-training. In this paper, we propose a meta adversarial learning approach to address this problem. The meta-learner will be guided to learn language-independent information by using an adversarial auxiliary objective of language identification, which makes the shared semantic space more compact and improves model generalization. Additionally, we optimize adversarial training using Wasserstein distance and temporal normalization, enabling more stable and simple training. Experiment results on IARPA BABEL and OpenSLR show a significant performance improvement. It also outperforms state-of-the-art results by a large margin in all target languages, and especially in few-shot settings. Finally, we demonstrate how our method is superior by using t-SNE visualization.},
  keywords = {Adversarial training,IARPA-BABEL,Low-resource,Meta learning,OpenSLR,Speech recognition},
  file = {C:\Users\Yu\Zotero\storage\JSRIG5G7\S0885230823000955.html}
}

@inproceedings{chenSandhiTonalShanghaien2024,
  title = {Sandhi tonal en shanghaïen : une étude acoustique des contours dissyllabiques chez des locuteurs jeunes},
  shorttitle = {Sandhi tonal en shanghaïen},
  author = {Chen, Yu and Vallée, Nathalie and Tran, Thi-Thuy-Hien and Gerber, Silvain},
  date = {2024-07-08},
  pages = {532},
  publisher = {ATALA \& AFPC},
  url = {https://inria.hal.science/hal-04623101},
  urldate = {2024-07-01},
  abstract = {Le shanghaïen possède deux types de sandhi tonal : Left Dominant Sandhi (LDS) dans les composés sémantiques de type syntagme nominal (SN) et Right Dominant Sandhi (RDS) dans des phrases prosodiques de type syntagme verbal (SV). Cette étude examine les caractéristiques acoustiques du contour tonal dans des SN et SV dissyllabiques chez trois locutrices jeunes. Nos résultats montrent que les tons des SN subissent des changements phonologiques relevant du LDS, alors que les SV sont plutôt soumis aux effets phonétiques de la coarticulation tonale plutôt qu’au RDS. L’absence de différences significatives entre les SN et les SV ne permet pas de généraliser une distinction entre eux uniquement sur la base des réalisations tonales. Cette étude exploratoire ouvre des perspectives pour de futurs travaux intergénérationnels sur les productions tonales et la perception du sandhi tonal, en étendant le corpus à différentes positions au sein de la phrase et différentes classes d’âge.},
  eventtitle = {35èmes Journées d'Études sur la Parole (JEP 2024) 31ème Conférence sur le Traitement Automatique des Langues Naturelles (TALN 2024) 26ème Rencontre des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (RECITAL 2024)},
  langid = {french},
  file = {C:\Users\Yu\Zotero\storage\3XZWMHGS\Chen et al. - 2024 - Sandhi tonal en shanghaïen  une étude acoustique .pdf}
}

@article{chodroffComparingLanguagespecificCrosslanguage2024,
  title = {Comparing Language-Specific and Cross-Language Acoustic Models for Low-Resource Phonetic Forced Alignment},
  author = {Chodroff, Eleanor and Ahn, Emily P and Dolatian, Hossep},
  date = {2024},
  journaltitle = {Language Documentation and Conservation},
  abstract = {Phonetic forced alignment can greatly expedite spoken language analysis by providing automatic time alignments at the word- and phone-levels. In the case of low-resource languages, it remains an open question whether phone-level forced alignment will be more successful with a small language-specific acoustic model or a high-resource cross-language acoustic model. The present study directly compared language-specific and cross-language acoustic models in forced alignment performance using the Urum and Evenki datasets from the DoReCo Corpus. We evaluated six language-specific acoustic models trained with 5, 10, 15, 20, 25, or approximately 70 minutes of language-specific speech data against four English-based cross-language acoustic models that differed in size and accent homogeneity (large Global English or homogeneous American English of varying data amounts). Acoustic models were developed or obtained from the Montreal Forced Aligner, and evaluated against held-out manually aligned phone boundaries. Overall, the Global English model and the larger language-specific acoustic models were competitive with one another, and outperformed the homogeneous cross-language and smaller language-specific acoustic models. From this analysis, we recommend that researchers use a language-specific model with at least 25 minutes of speech (not just audio) or a large, diverse cross-language acoustic model for low-resource forced alignment.},
  langid = {english},
  file = {C:\Users\Yu\Zotero\storage\3QG7Z6UU\Chodroff 等 - Comparing language-specific and cross-language aco.pdf}
}

@online{chorowskiAttentionBasedModelsSpeech2015,
  title = {Attention-{{Based Models}} for {{Speech Recognition}}},
  author = {Chorowski, Jan and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2015-06-24},
  eprint = {1506.07503},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1506.07503},
  urldate = {2024-06-13},
  abstract = {Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1, 2] and image caption generation [3]. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in [2] reaches a competitive 18.7\% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18\% PER in single utterances and 20\% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6\% level.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C:\Users\Yu\Zotero\storage\CKMRHUF9\1506.pdf}
}

@book{cookDiccionarioBilingueKoreguajeespanol2001,
  title = {Diccionario bilingüe koreguaje-español, español-koreguaje},
  author = {Cook, Dorothy M. and Gralow, Frances L. and Muller de Young, Carolyn},
  date = {2001},
  edition = {1. ed},
  publisher = {Editorial Alberto Lleras Camargo},
  location = {Santafé de Bogotá},
  isbn = {978-958-9281-50-5},
  langid = {spasai},
  pagetotal = {203},
  keywords = {Coreguaje,Coreguaje language,Dictionaries,Spanish,Spanish language}
}

@mvbook{cookIdiomaKoreguajeTucano1993,
  title = {El idioma koreguaje (tucano occidental)},
  author = {Cook, Dorothy M. and Criswell, Linda},
  date = {1993},
  edition = {1. ed},
  publisher = {Asociación Instituto Lingüístico de Verano},
  location = {Santafé de Bogotá, Colombia},
  url = {http://books.google.com/books?id=DgIvAAAAYAAJ},
  langid = {spanish},
  volumes = {1 online resource (111 pages)}
}

@article{davisAutomaticRecognitionSpoken1952,
  title = {Automatic {{Recognition}} of {{Spoken Digits}}},
  author = {Davis, K. H. and Biddulph, R. and Balashek, S.},
  date = {1952-11-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {24},
  number = {6},
  pages = {637--642},
  issn = {0001-4966, 1520-8524},
  doi = {10.1121/1.1906946},
  url = {https://pubs.aip.org/jasa/article/24/6/637/618458/Automatic-Recognition-of-Spoken-Digits},
  urldate = {2024-06-12},
  abstract = {The recognizer discussed will automatically recognize telephone-quality digits spoken at normal speech rates by a single individual, with an accuracy varying between 97 and 99 percent. After some preliminary analysis of the speech of any individual, the circuit can be adjusted to deliver a similar accuracy on the speech of that individual. The circuit is not, however, in its present configuration, capable of performing equally well on the speech of a series of talkers without recourse to such adjustment.             Circuitry involves division of the speech spectrum into two frequency bands, one below and the other above 900 cps. Axis-crossing counts are then individually made of both band energies to determine the frequency of the maximum syllabic rate energy with each band. Simultaneous two-dimensional frequency portrayal is found to possess recognition significance. Standards are then determined, one for each digit of the ten-digit series, and are built into the recognizer as a form of elemental memory. By means of a series of calculations performed automatically on the spoken input digit, a best match type comparison is made with each of the ten standard digit patterns and the digit of best match selected.},
  langid = {english}
}

@article{dhanjalComprehensiveSurveyAutomatic2023,
  title = {A Comprehensive Survey on Automatic Speech Recognition Using Neural Networks},
  author = {Dhanjal, Amandeep Singh and Singh, Williamjeet},
  date = {2023-08-15},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {83},
  number = {8},
  pages = {23367--23412},
  issn = {1573-7721},
  doi = {10.1007/s11042-023-16438-y},
  url = {https://link.springer.com/10.1007/s11042-023-16438-y},
  urldate = {2024-06-04},
  abstract = {The continuous development in Automatic Speech Recognition has grown and demonstrated its enormous potential in Human Interaction Communication systems. It is quite a challenging task to achieve high accuracy due to several parameters such as different dialects, spontaneous speech, speaker’s enrolment, computation power, dataset, and noisy environment that decrease the performance of the speech recognition system. It has motivated various researchers to make innovative contributions to the development of a robust speech recognition system. The study presents a systematic analysis of current state-of-the-art research work done in this field during 2015-2021. The prime focus of the study is to highlight the neural network-based speech recognition techniques, datasets, toolkits, and evaluation metrics utilized in the past seven years. It also synthesizes the evidence from past studies to provide empirical solutions for accuracy improvement. This study highlights the current status of speech recognition systems using neural networks and provides a brief knowledge to the new researchers.},
  langid = {english},
  file = {C:\Users\Yu\Zotero\storage\SWDC9D97\Dhanjal and Singh - 2023 - A comprehensive survey on automatic speech recogni.pdf}
}

@book{dibAutomaticSpeechRecognition2019,
  title = {Automatic {{Speech Recognition}} of {{Arabic Phonemes}} with {{Neural Networks}}: {{A Contrastive Study}} of {{Arabic}} and {{English}}},
  shorttitle = {Automatic {{Speech Recognition}} of {{Arabic Phonemes}} with {{Neural Networks}}},
  author = {Dib, Mohammed},
  date = {2019},
  series = {{{SpringerBriefs}} in {{Applied Sciences}} and {{Technology}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-97710-2},
  url = {http://link.springer.com/10.1007/978-3-319-97710-2},
  urldate = {2024-06-04},
  isbn = {978-3-319-97709-6 978-3-319-97710-2},
  langid = {english},
  file = {C:\Users\Yu\Zotero\storage\XAAJ3RAN\Dib - 2019 - Automatic Speech Recognition of Arabic Phonemes wi.pdf}
}

@article{dupontmorenoEstructuraSonoraKoreguaje2021,
  title = {La {{Estructura Sonora}} Del {{Koreguaje}}: Segmentos, Supra-Segmentos y Proto-Segmentos Asociados a La Armonía Nasal En La Lengua Koreguaje (Tukano Occidental)},
  shorttitle = {La {{Estructura Sonora}} Del {{Koreguaje}}},
  author = {Dupont Moreno, Carlos},
  date = {2021-12-29},
  journaltitle = {Revista Brasileira de Linguística Antropológica},
  shortjournal = {Rev. Bras. de Ling. Antrop.},
  volume = {13},
  pages = {347--366},
  issn = {2317-1375, 2176-834X},
  doi = {10.26512/rbla.v13i01.40360},
  url = {https://periodicos.unb.br/index.php/ling/article/view/40360},
  urldate = {2024-06-17},
  abstract = {La lengua koreguaje, perteneciente a la subfamilia de lenguas tukanas occidentales, se habla en el suroccidente de Colombia por alrededor de 2.500 personas. Una característica importante de su estructura sonora es la nasalidad, la cual tratamos en este trabajo en un nivel suprasegmental. Para la descripción de los procesos de expansión de la nasalidad al interior de los morfemas y morfo-fonemas, adoptamos el marco teórico de la fonología lexical. Estos procesos se efectúan a partir de representaciones subyacentes, sustentadas por estudios proto-fonémicos, de las unidades mínimas significativas de la lengua para derivar sus estructuras fonéticas. A través de ejemplificaciones, intentamos demostrar la validez de esta propuesta teórica.},
  file = {C:\Users\Yu\Zotero\storage\LJIAUCSV\Dupont Moreno - 2021 - La Estructura Sonora del Koreguaje segmentos, sup.pdf}
}

@software{Espnet2024,
  title = {Espnet},
  date = {2024-06-14T10:27:41Z},
  origdate = {2017-12-13T00:45:11Z},
  url = {https://github.com/espnet/espnet},
  urldate = {2024-06-14},
  abstract = {End-to-End Speech Processing Toolkit},
  organization = {ESPnet},
  keywords = {chainer,deep-learning,end-to-end,kaldi,machine-translation,pytorch,singing-voice-synthesis,speaker-diarization,speech-enhancement,speech-recognition,speech-separation,speech-synthesis,speech-translation,spoken-language-understanding,text-to-speech,voice-conversion}
}

@online{GIPSALab2024,
  title = {{{GIPSA-Lab}}},
  date = {2024},
  url = {https://www.gipsa-lab.grenoble-inp.fr/},
  urldate = {2024-05-26},
  organization = {Gipsa-lab},
  file = {C:\Users\Yu\Zotero\storage\UYJGXW5M\www.gipsa-lab.grenoble-inp.fr.html}
}

@inproceedings{gravesConnectionistTemporalClassification2006,
  title = {Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks},
  shorttitle = {Connectionist Temporal Classification},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning  - {{ICML}} '06},
  author = {Graves, Alex and Fernández, Santiago and Gomez, Faustino and Schmidhuber, Jürgen},
  date = {2006},
  pages = {369--376},
  publisher = {ACM Press},
  location = {Pittsburgh, Pennsylvania},
  doi = {10.1145/1143844.1143891},
  url = {http://portal.acm.org/citation.cfm?doid=1143844.1143891},
  urldate = {2024-06-13},
  eventtitle = {The 23rd International Conference},
  isbn = {978-1-59593-383-6},
  langid = {english},
  file = {C:\Users\Yu\Zotero\storage\UCPMSP2D\Graves et al. - Connectionist Temporal Classiﬁcation Labelling Un.pdf}
}

@inproceedings{gulatiConformerConvolutionaugmentedTransformer2020,
  title = {Conformer: {{Convolution-augmented Transformer}} for {{Speech Recognition}}},
  shorttitle = {Conformer},
  booktitle = {Interspeech 2020},
  author = {Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and Pang, Ruoming},
  date = {2020-10-25},
  pages = {5036--5040},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2020-3015},
  url = {https://www.isca-archive.org/interspeech_2020/gulati20_interspeech.html},
  urldate = {2024-06-14},
  eventtitle = {Interspeech 2020},
  langid = {english},
  file = {C:\Users\Yu\Zotero\storage\WGGYDPU7\Gulati et al. - 2020 - Conformer Convolution-augmented Transformer for S.pdf}
}

@article{guptaHybridDeepLearning2023,
  title = {Hybrid Deep Learning Based Automatic Speech Recognition Model for Recognizing Non-{{Indian}} Languages},
  author = {Gupta, Astha and Kumar, Rakesh and Kumar, Yogesh},
  date = {2023-09-15},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {83},
  number = {10},
  pages = {30145--30166},
  issn = {1573-7721},
  doi = {10.1007/s11042-023-16748-1},
  url = {https://link.springer.com/10.1007/s11042-023-16748-1},
  urldate = {2024-06-04},
  abstract = {Speech is a natural phenomenon and a significant mode of communication used by humans that is divided into two categories, human-to-human and human-to-machine. Human-tohuman communication depends on the language the speaker uses. In contrast, human-tomachine communication is a technique in which machines recognize human speech and act accordingly, often termed Automatic Speech Recognition (ASR). Recognition of NonIndian language is challenging due to pitch variations and other factors such as accent, pronunciation, etc. This paper proposes a novel approach based on Dense Net201 and EfficientNetB0, i.e., a hybrid model for the recognition of Speech. Initially, 76,263 speech samples are taken from 11 non-Indian languages, including Chinese, Dutch, Finnish, French, German, Greek, Hungarian, Japanese, Russian, Spanish and Persian. When collected, these speech samples are pre-processed by removing noise. Then, Spectrogram, Short-Term Fourier Transform (STFT), Spectral Rolloff-Bandwidth, Mel-frequency Cepstral Coefficient (MFCC), and Chroma feature are used to extract features from the speech sample. Further, a comparative analysis of the proposed approach is shown with other Deep Learning (DL) models like ResNet10, Inception V3, VGG16, DenseNet201, and EfficientNetB0. Standard parameters like Precision, Recall, F1-Score, Confusion Matrix, Accuracy, and Loss curves are used to evaluate the performance of each model by considering speech samples from all the languages mentioned above. Thus, the experimental results show that the hybrid model stands out from all the other models by giving the highest recognition accuracy of 99.84\% with a loss of 0.004\%.},
  langid = {english},
  file = {C:\Users\Yu\Zotero\storage\F6EUJQG9\Gupta et al. - 2023 - Hybrid deep learning based automatic speech recogn.pdf}
}

@software{heafieldKenLm2024,
  title = {{{KenLm}}},
  author = {Heafield, Kenneth},
  date = {2024},
  url = {https://github.com/kpu/kenlm},
  abstract = {KenLM: Faster and Smaller Language Model Queries}
}

@online{hsuHuBERTSelfSupervisedSpeech2021,
  title = {{{HuBERT}}: {{Self-Supervised Speech Representation Learning}} by {{Masked Prediction}} of {{Hidden Units}}},
  shorttitle = {{{HuBERT}}},
  author = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  date = {2021-06-14},
  eprint = {2106.07447},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2106.07447},
  urldate = {2024-06-14},
  abstract = {Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19\% and 13\% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C:\Users\Yu\Zotero\storage\N5RY25SW\2106.pdf}
}

@online{Hubert,
  title = {Hubert},
  url = {https://github.com/facebookresearch/fairseq/tree/main/examples/hubert},
  urldate = {2024-06-14},
  abstract = {Facebook AI Research Sequence-to-Sequence Toolkit written in Python. - facebookresearch/fairseq},
  langid = {english},
  organization = {GitHub},
  file = {C:\Users\Yu\Zotero\storage\4THM2R86\hubert.html}
}

@online{huggingfaceHuggingFaceAI2024,
  title = {Hugging {{Face}} – {{The AI}} Community Building the Future.},
  author = {{Hugging Face}},
  date = {2024-06-19},
  url = {https://huggingface.co/},
  urldate = {2024-06-24},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {C:\Users\Yu\Zotero\storage\8ILCKKXS\huggingface.co.html}
}

@online{Kaldi,
  title = {Kaldi},
  url = {https://github.com/kaldi-asr/kaldi},
  urldate = {2024-06-14},
  file = {C:\Users\Yu\Zotero\storage\HBMU7CHL\kaldi.html}
}

@article{kenfackjeuguimYembaTonesSyllabletoneAnnotated2024,
  title = {{{YembaTones}}: {{A}} Syllable-Tone Annotated Dataset for Speech Recognition and Prosodic Analysis of the {{Yemba}} Language},
  shorttitle = {{{YembaTones}}},
  author = {Kenfack Jeuguim, Marc Sturm and Melatagia Yonta, Paulin and Sandembouo, Etienne},
  date = {2024-02-01},
  journaltitle = {Data in Brief},
  shortjournal = {Data in Brief},
  volume = {52},
  pages = {109860},
  issn = {2352-3409},
  doi = {10.1016/j.dib.2023.109860},
  url = {https://www.sciencedirect.com/science/article/pii/S2352340923009216},
  urldate = {2024-06-04},
  abstract = {Prosody is a key area of linguistics that explores tonal and rhythmic variations in speech. In tonal languages such as Yemba, prosody plays a crucial role in distinguishing between words with different meanings or different grammatical forms. However, despite the large number of native speakers of this language in Cameroon, there are few resources for the speech recognition and synthesis. In this article, we present YembaTones, a syllabic and tonal annotated dataset, created from a dictionary we designed of 344 Yemba/French words coming from the most common phrases of the language, grouped according to their spellings that only differ by the tone. The dataset was originally designed for training and evaluating tone detection models for tonal and low resource languages. The recordings of the pronunciation of these words were made with 11 native speakers of Yemba, mainly specialists in linguistics with a good command of the sounds of the language. The recordings were made with a dictaphone in different places such as the homes of the speakers, the campuses and their workplaces. Then they have been cleaned and segmented into individual audio files corresponding to the pronunciations of isolated words, using the software Audacity. After cleaning and segmentation, we selected 3420 good quality audio files for annotation. Annotations were made at the syllabic and tonal level using Praat software. YembaTones is a valuable resource not only for the training and evaluation of automatic tone detection models but also for automatic speech recognition, speech synthesis of tonal and poorly endowed languages, as well as for the study of prosody and Yemba phonetics, research in speech acoustics and phonetic linguistics.},
  keywords = {Annotated data,Automatic speech processing,Low resource languages,Prosody,Speech recognition,Tones detection,Vocal synthesis,Yemba},
  file = {C\:\\Users\\Yu\\Zotero\\storage\\A8NJAIX3\\Kenfack Jeuguim et al. - 2024 - YembaTones A syllable-tone annotated dataset for .pdf;C\:\\Users\\Yu\\Zotero\\storage\\NYJVCFHY\\S2352340923009216.html}
}

@article{kimanukaSpeechRecognitionDatasets2024,
  title = {Speech Recognition Datasets for Low-Resource {{Congolese}} Languages},
  author = {Kimanuka, Ussen and family=Maina, given=Ciira, prefix=wa, useprefix=false and Büyük, Osman},
  date = {2024-02-01},
  journaltitle = {Data in Brief},
  shortjournal = {Data in Brief},
  volume = {52},
  pages = {109796},
  issn = {2352-3409},
  doi = {10.1016/j.dib.2023.109796},
  url = {https://www.sciencedirect.com/science/article/pii/S2352340923008582},
  urldate = {2024-06-04},
  abstract = {Large pre-trained Automatic Speech Recognition (ASR) models have shown improved performance in low-resource languages due to the increased availability of benchmark corpora and the advantages of transfer learning. However, only a limited number of languages possess ample resources to fully leverage transfer learning. In such contexts, benchmark corpora become crucial for advancing methods. In this article, we introduce two new benchmark corpora designed for low-resource languages spoken in the Democratic Republic of the Congo: the Lingala Read Speech Corpus, with 4 h of labelled audio, and the Congolese Speech Radio Corpus, which offers 741 h of unlabelled audio spanning four significant low-resource languages of the region. During data collection, Lingala Read Speech recordings of thirty-two distinct adult speakers, each with a unique context under various settings with different accents, were recorded. Concurrently, Congolese Speech Radio raw data were taken from the archive of broadcast station, followed by a designed curation process. During data preparation, numerous strategies have been utilised for pre-processing the data. The datasets, which have been made freely accessible to all researchers, serve as a valuable resource for not only investigating and developing monolingual methods and approaches that employ linguistically distant languages but also multilingual approaches with linguistically similar languages. Using techniques such as supervised learning and self-supervised learning, they are able to develop inaugural benchmarking of speech recognition systems for Lingala and mark the first instance of a multilingual model tailored for four Congolese languages spoken by an aggregated population of 95 million. Moreover, two models were applied to this dataset. The first is supervised learning modelling and the second is for self-supervised pre-training.},
  keywords = {Automatic speech recognition,Cross-lingual acoustic model,Multilingual acoustic model,Pre-trained models,Self-supervised learning,Transfer learning},
  file = {C\:\\Users\\Yu\\Zotero\\storage\\VXPA6SS8\\Kimanuka et al. - 2024 - Speech recognition datasets for low-resource Congo.pdf;C\:\\Users\\Yu\\Zotero\\storage\\Z8XV5GMH\\S2352340923008582.html}
}

@online{KorebajuArtProject2024,
  title = {Korebaju {{Art Project}}},
  date = {2024},
  url = {https://www.korebajuartproject.com},
  urldate = {2024-05-31},
  organization = {Korebaju Art Project},
  file = {C:\Users\Yu\Zotero\storage\7NZWWJ9J\www.korebajuartproject.com.html}
}

@book{kumarAutomaticSpeechRecognition2024,
  title = {Automatic Speech Recognition and Translation for Low Resource Languages},
  editor = {Kumar, L. Ashok},
  date = {2024},
  publisher = {John Wiley \& Sons, Inc. ; Scrivener Publishing LLC},
  location = {Hoboken, NJ, Beverly, MA},
  abstract = {AUTOMATIC SPEECH RECOGNITION and TRANSLATION for LOW-RESOURCE LANGUAGES This book is a comprehensive exploration into the cutting-edge research, methodologies, and advancements in addressing the unique challenges associated with ASR and translation for low-resource languages. Automatic Speech Recognition and Translation for Low Resource Languages contains groundbreaking research from experts and researchers sharing innovative solutions that address language challenges in low-resource environments. The book begins by delving into the fundamental concepts of ASR and translation, providing readers with a solid foundation for understanding the subsequent chapters. It then explores the intricacies of low-resource languages, analyzing the factors that contribute to their challenges and the significance of developing tailored solutions to overcome them. The chapters encompass a wide range of topics, ranging from both the theoretical and practical aspects of ASR and translation for low-resource languages. The book discusses data augmentation techniques, transfer learning, and multilingual training approaches that leverage the power of existing linguistic resources to improve accuracy and performance. Additionally, it investigates the possibilities offered by unsupervised and semi-supervised learning, as well as the benefits of active learning and crowdsourcing in enriching the training data. Throughout the book, emphasis is placed on the importance of considering the cultural and linguistic context of low-resource languages, recognizing the unique nuances and intricacies that influence accurate ASR and translation. Furthermore, the book explores the potential impact of these technologies in various domains, such as healthcare, education, and commerce, empowering individuals and communities by breaking down language barriers. Audience The book targets researchers and professionals in the fields of natural language processing, computational linguistics, and speech technology. It will also be of interest to engineers, linguists, and individuals in industries and organizations working on cross-lingual communication, accessibility, and global connectivity},
  isbn = {978-1-394-21462-4},
  langid = {english},
  annotation = {OCLC: 1427496804},
  file = {C:\Users\Yu\Zotero\storage\4GP4U7R2\Automatic Speech Recognition and Translation for Low Resource Languages (for True Epub) ( etc.) (Z-Library).pdf}
}

@book{leeAutomaticSpeechRecognition1989,
  title = {Automatic {{Speech Recognition}}},
  author = {Lee, Kai-Fu},
  date = {1989},
  publisher = {Springer US},
  location = {Boston, MA},
  doi = {10.1007/978-1-4615-3650-5},
  url = {http://link.springer.com/10.1007/978-1-4615-3650-5},
  urldate = {2024-06-04},
  isbn = {978-1-4613-6624-9 978-1-4615-3650-5},
  langid = {english},
  file = {C:\Users\Yu\Zotero\storage\5UXQNG9J\Lee - 1989 - Automatic Speech Recognition.pdf}
}

@article{liChineseDialectSpeech2024,
  title = {Chinese Dialect Speech Recognition: A Comprehensive Survey},
  shorttitle = {Chinese Dialect Speech Recognition},
  author = {Li, Qiang and Mai, Qianyu and Wang, Mandou and Ma, Mingjuan},
  date = {2024-01-31},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  volume = {57},
  number = {2},
  pages = {25},
  issn = {1573-7462},
  doi = {10.1007/s10462-023-10668-0},
  url = {https://doi.org/10.1007/s10462-023-10668-0},
  urldate = {2024-06-04},
  abstract = {As a multi-ethnic country with a large population, China is endowed with diverse dialects, which brings considerable challenges to speech recognition work. In fact, due to geographical location, population migration, and other factors, the research progress and practical application of Chinese dialect speech recognition are currently at different stages. Therefore, exploring the significant regional heterogeneities in specific recognition approaches and effects, dialect corpus, and other resources is of vital importance for Chinese speech recognition work. Based on this, we first start with the regional classification of dialects and analyze the pivotal acoustic characteristics of dialects, including specific vowels and tones patterns. Secondly, we comprehensively summarize the existing dialect phonetic corpus in China, which is of some assistance in exploring the general construction methods of dialect phonetic corpus. Moreover, we expound on the general process of dialect recognition. Several critical dialect recognition approaches are summarized and introduced in detail, especially the hybrid method of Artificial Neural Network (ANN) combined with the Hidden Markov Model(HMM), as well as the End-to-End (E2E). Thirdly, through the in-depth comparison of their principles, merits, disadvantages, and  recognition performance for different dialects, the development trends and challenges in dialect recognition in the future are pointed out. Finally, some application examples of dialect speech recognition are collected and discussed.},
  langid = {english},
  keywords = {Automatic speech recognition,Chinese dialect,Deep neural network,Dialect corpus,Dialectal acoustic modeling,End-to-end},
  file = {C:\Users\Yu\Zotero\storage\FT5ZQABS\Li et al. - 2024 - Chinese dialect speech recognition a comprehensiv.pdf}
}

@article{liESAformerEnhancedSelfAttention2024,
  title = {{{ESAformer}}: {{Enhanced Self-Attention}} for {{Automatic Speech Recognition}}},
  shorttitle = {{{ESAformer}}},
  author = {Li, Junhua and Duan, Zhikui and Li, Shiren and Yu, Xinmei and Yang, Guangguang},
  date = {2024},
  journaltitle = {IEEE Signal Processing Letters},
  shortjournal = {IEEE Signal Process. Lett.},
  volume = {31},
  pages = {471--475},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2024.3358754},
  url = {https://ieeexplore.ieee.org/document/10414120/},
  urldate = {2024-06-04}
}

@online{LIGUniversiteGrenoble2024,
  title = {{{LIG}} - {{Université Grenoble Alpes}}},
  date = {2024},
  url = {https://www.liglab.fr/fr},
  urldate = {2024-05-31},
  file = {C:\Users\Yu\Zotero\storage\5P6VLDQN\fr.html}
}

@online{liRecentAdvancesEndtoEnd2021,
  title = {Recent {{Advances}} in {{End-to-End Automatic Speech Recognition}}},
  author = {Li, Jinyu},
  date = {2021},
  doi = {10.48550/ARXIV.2111.01690},
  url = {https://arxiv.org/abs/2111.01690},
  urldate = {2024-06-12},
  abstract = {Recently, the speech community is seeing a significant trend of moving from deep neural network based hybrid modeling to end-to-end (E2E) modeling for automatic speech recognition (ASR). While E2E models achieve the state-of-the-art results in most benchmarks in terms of ASR accuracy, hybrid models are still used in a large proportion of commercial ASR systems at the current time. There are lots of practical factors that affect the production model deployment decision. Traditional hybrid models, being optimized for production for decades, are usually good at these factors. Without providing excellent solutions to all these factors, it is hard for E2E models to be widely commercialized. In this paper, we will overview the recent advances in E2E models, focusing on technologies addressing those challenges from the industry's perspective.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Artificial Intelligence (cs.AI),Audio and Speech Processing (eess.AS),Computation and Language (cs.CL),Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Sound (cs.SD)},
  file = {C:\Users\Yu\Zotero\storage\AB5TJLM4\Li - 2022 - Recent Advances in End-to-End Automatic Speech Rec.pdf}
}

@thesis{luResearchApplicationSpeech2023,
  type = {mathesis},
  title = {Research and application of speech recognition based on Conformer},
  author = {Lu, Jiangkun},
  date = {2023},
  institution = {Shandong Jianzhu University},
  location = {Jinan},
  langid = {chinese},
  file = {C:\Users\Yu\Zotero\storage\FGMNM5UY\基于Conformer的语音识别研究与应用_卢江坤.pdf}
}

@software{MontrealCorpusToolsMontrealForcedAligner2024,
  title = {{{MontrealCorpusTools}}/{{Montreal-Forced-Aligner}}},
  date = {2024-09-04T05:25:16Z},
  origdate = {2015-10-26T17:02:06Z},
  url = {https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner},
  urldate = {2024-09-04},
  abstract = {Command line utility for forced alignment using Kaldi},
  organization = {MontrealCorpusTools},
  keywords = {acoustic-model,forced-alignment,grapheme-to-phone,kaldi,pronunciation-dictionary,python}
}

@online{MontrealForcedAligner2024,
  title = {Montreal {{Forced Aligner}}},
  date = {2024},
  url = {https://montreal-forced-aligner.readthedocs.io/en/latest/#},
  urldate = {2024-09-04},
  file = {C:\Users\Yu\Zotero\storage\CGATA6U2\latest.html}
}

@online{MPSIMultimodalPerception2024,
  title = {M-{{PSI}} – {{Multimodal Perception}} and {{Sociable Interaction}}},
  date = {2024},
  url = {https://www.m-psi.fr/},
  urldate = {2024-05-31},
  langid = {british},
  file = {C:\Users\Yu\Zotero\storage\VHGC6REG\www.m-psi.fr.html}
}

@inproceedings{myoSyllableBasedMyanmarSpeechtoText2024,
  title = {Syllable-{{Based Myanmar Speech-to-Text ASR}} Using {{Deep Learning}} and {{CTC}}},
  booktitle = {2024 {{Conference}} of {{Young Researchers}} in {{Electrical}} and {{Electronic Engineering}} ({{ElCon}})},
  author = {Myo, Saw Kyaw Htin and Thet, Moe and Htun, Hein and Myint, Arkar and Htut, La Min and Shie, Htut},
  date = {2024-01-29},
  pages = {281--285},
  publisher = {IEEE},
  location = {Saint Petersburg, Russian Federation},
  doi = {10.1109/ElCon61730.2024.10468151},
  url = {https://ieeexplore.ieee.org/document/10468151/},
  urldate = {2024-06-04},
  eventtitle = {2024 {{Conference}} of {{Young Researchers}} in {{Electrical}} and {{Electronic Engineering}} ({{ElCon}})},
  isbn = {9798350360646}
}

@online{NumPy2024,
  title = {{{NumPy}}},
  date = {2024},
  url = {https://numpy.org/},
  urldate = {2024-09-04},
  file = {C:\Users\Yu\Zotero\storage\C6NTJYEG\numpy.org.html}
}

@inproceedings{parikhComparingModularEndEnd2023,
  title = {Comparing {{Modular}} and {{End-To-End Approaches}} in {{ASR}} for {{Well-Resourced}} and {{Low-Resourced Languages}}},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Natural Language}} and {{Speech Processing}} ({{ICNLSP}} 2023)},
  author = {Parikh, Aditya and family=Bosch, given=Louis, prefix=ten, useprefix=true and family=Heuvel, given=Henk, prefix=van den, useprefix=true and Tejedor-Garcia, Cristian},
  editor = {Abbas, Mourad and Freihat, Abed Alhakim},
  date = {2023},
  pages = {266--273},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  url = {https://aclanthology.org/2023.icnlsp-1.28},
  urldate = {2024-09-04},
  eventtitle = {{{ICNLSP}} 2023},
  file = {C:\Users\Yu\Zotero\storage\W4DUCA7G\Parikh 等 - 2023 - Comparing Modular and End-To-End Approaches in ASR.pdf}
}

@article{pipirasLithuanianSpeechRecognition2019,
  title = {Lithuanian {{Speech Recognition Using Purely Phonetic Deep Learning}}},
  author = {Pipiras, Laurynas and Maskeliūnas, Rytis and Damaševičius, Robertas},
  date = {2019-12},
  journaltitle = {Computers},
  volume = {8},
  number = {4},
  pages = {76},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2073-431X},
  doi = {10.3390/computers8040076},
  url = {https://www.mdpi.com/2073-431X/8/4/76},
  urldate = {2024-05-29},
  abstract = {Automatic speech recognition (ASR) has been one of the biggest and hardest challenges in the field. A large majority of research in this area focuses on widely spoken languages such as English. The problems of automatic Lithuanian speech recognition have attracted little attention so far. Due to complicated language structure and scarcity of data, models proposed for other languages such as English cannot be directly adopted for Lithuanian. In this paper we propose an ASR system for the Lithuanian language, which is based on deep learning methods and can identify spoken words purely from their phoneme sequences. Two encoder-decoder models are used to solve the ASR task: a traditional encoder-decoder model and a model with attention mechanism. The performance of these models is evaluated in isolated speech recognition task (with an accuracy of 0.993) and long phrase recognition task (with an accuracy of 0.992).},
  issue = {4},
  langid = {english},
  keywords = {artificial neural networks,deep learning,Lithuanian speech recognition,phonetic encoder-decoder models},
  file = {C:\Users\Yu\Zotero\storage\GEHEH7R6\Pipiras 等 - 2019 - Lithuanian Speech Recognition Using Purely Phoneti.pdf}
}

@inproceedings{poveyKaldiSpeechRecognition2011,
  title = {The {{Kaldi Speech Recognition Toolkit}}},
  booktitle = {{{IEEE}} 2011 Workshop on Automatic Speech Recognition and Understanding},
  author = {Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, Lukasˇ and Glembek, Ondˇrej and Goel, Nagendra and Hannemann, Mirko and Motlıcˇek, Petr and Qian, Yanmin and Schwarz, Petr and Silovsky, Jan and Stemmer, Georg and Vesely, Karel},
  date = {2011},
  publisher = {IEEE Signal Processing Society},
  abstract = {We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state transducers (using the freely available OpenFst), together with detailed documentation and scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms. Kaldi is released under the Apache License v2.0, which is highly nonrestrictive, making it suitable for a wide community of users.},
  langid = {english},
  file = {C:\Users\Yu\Zotero\storage\PQ8GTK6U\Povey 等 - The Kaldi Speech Recognition Toolkit.pdf}
}

@article{rabinerTutorialHiddenMarkov1989,
  title = {A {{Tutorial}} on {{Hidden Markov Models}} and {{Selected Applications}} in {{Speech Recognition}}},
  author = {Rabiner, Lawrence R},
  date = {1989},
  journaltitle = {PROCEEDINGS OF THE IEEE},
  volume = {77},
  number = {2},
  langid = {english},
  file = {C:\Users\Yu\Zotero\storage\IJS2J4EP\Rabiner - 1989 - A Tutorial on Hidden Markov Models and Selected Ap.pdf}
}

@software{robertPydub2024,
  title = {Pydub},
  author = {Robert, James},
  date = {2024},
  url = {https://github.com/jiaaro/pydub},
  abstract = {Manipulate audio with a simple and easy high level interface}
}

@inproceedings{rodriguezGlottalisationKorebajuQuestion2022,
  title = {Glottalisation En {{Korebaju}} : La Question d'un Trait Mixte Segmental et Suprasegmental},
  shorttitle = {Glottalisation En {{Korebaju}}},
  author = {Rodriguez, Jenifer Vega and Vallée, Nathalie and Chacon, Thiago},
  date = {2022-04-03},
  doi = {10.21437/JEP.2022-84},
  abstract = {Le korebaju (ISO 639-3 : coe)(koreguaje) est une langue tonale de la famille Tukano de l’Ouest, parlée le long des rivières Orteguaza, Peneya et Consaya, dans les départements de Caquetá et de Putumayo, au sud-ouest de la Colombie. Le korebaju [kòrèβàhɨ] ́ comporte un système vocalique de six voyelles orales /i, e, a, o, u, ɨ/, six voyelles nasales /ĩ, ẽ, ã, õ, ũ, ɨ̃/ et trois voyelles glottales /aˀ/, /eˀ/ et /oˀ/, ainsi que 17 consonnes /p t k pʰ tʰ kʰ β ɸ s h w t͡ʃ m n ɲ ʰɲ r/. Cette langue compte environ 2 000 locuteurs et locutrices répartis en 27 villages. Cette communauté est actuellement le produit de la fusion de quatre communautés : Korebaju, Tama, Macaguaje et Carijona. Ainsi, on constate des variétés dialectales selon l’ascendance des clans : carijona, tama, macaguaje et korebaju. La glottalisation est un sujet très controversé en korebaju, comme dans les autres langues tukano et de leur ancêtre commun. Quatre analyses ont été proposées pour la glottalisation allant de la production spontanée par les locuteurs sans aucun caractère contrastif ; en passant par une analyse autosegmentale proposant une propriété multilinéaire ; jusqu’à un statut phonémique /ʔ/.L’étude que nous présentons ici cherche à vérifier l’hypothèse du double statut phonologique (segmental et suprasegmental) proposé pour cette langue, hypothèse que nous avons élaborée à partir de l’examen de trois voyelles glottales distinctives /eˀ, aˀ, oˀ/, comme fait antérieurement. Nousproposons de vérifier, à partir de données originales et pour ces unités phonémiques, si la glottalisation est un trait du niveau suprasegmental qui surface comme attaque syllabique, ou s’il s’agit plutôt d’un trait segmental d’une articulation élaborée. Nous posons alors la question de l’existencepossible d’un système mixte dans cette langue comprenant, d’un côté, des segments vocaliques portant le trait [constricted glottis] et, de l’autre, une glottalisation suprasegmentale conditionnée par un contexte CVV.},
  file = {C:\Users\Yu\Zotero\storage\8HNEC6P9\Vega Rodriguez 等 - 2022 - Glottalisation en Korebaju  la question d'un trai.pdf}
}

@inproceedings{rodriguezGlottalSoundsKorebaju2021,
  title = {Glottal {{Sounds}} in {{Korebaju}}},
  booktitle = {Interspeech 2021},
  author = {Rodriguez, Jenifer Vega and Vallée, Nathalie},
  date = {2021-08-30},
  pages = {1011--1014},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2021-1417},
  url = {https://www.isca-archive.org/interspeech_2021/rodriguez21_interspeech.html},
  urldate = {2024-05-10},
  eventtitle = {Interspeech 2021},
  langid = {english}
}

@inproceedings{rodriguezIntraInterDialectalStudy2023,
  title = {An {{Intra-}} and {{Inter-Dialectal Study}} of {{Korebaju Vowels}}},
  booktitle = {2nd {{Annual Meeting}} of the {{ELRA}}/{{ISCA SIG}} on {{Under-resourced Languages}} ({{SIGUL}} 2023)},
  author = {Rodriguez, Jenifer Vega and Vallée, Nathalie and Chacon, Thiago and Savariaux, Christophe and Gerber, Silvain},
  date = {2023-08-18},
  pages = {24--28},
  publisher = {ISCA},
  doi = {10.21437/SIGUL.2023-6},
  url = {https://www.isca-archive.org/sigul_2023/vegarodriguez23_sigul.html},
  urldate = {2024-05-10},
  eventtitle = {2nd {{Annual Meeting}} of the {{ELRA}}/{{ISCA SIG}} on {{Under-resourced Languages}} ({{SIGUL}} 2023)},
  langid = {english}
}

@inproceedings{rodriguezSegmentationAnalyseProductions2023,
  title = {Segmentation et Analyse de Productions Non Modales, Une Étude de Cas : La Langue Korebaju},
  shorttitle = {Segmentation et Analyse de Productions Non Modales, Une Étude de Cas},
  booktitle = {5èmes Journées Du {{Groupement}} de {{Recherche CNRS}} “ {{Linguistique Informatique}}, {{Formelle}} et de {{Terrain}} ” {{LIFT}} 2023},
  author = {Rodriguez, Jenifer Vega and Vallée, Nathalie},
  date = {2023-11},
  series = {Actes Des 5es Journées Du {{GDR CNRS}} “ {{Linguistique Informatique}}, {{Formelle}} et de {{Terrain}} ”},
  pages = {23--29},
  location = {Nancy, France},
  url = {https://hal.science/hal-04380198},
  urldate = {2024-05-31},
  abstract = {Le korebaju (koreguaje) est une langue tonale appartenant à la famille Tukano parlée dans l’Amazonie colombienne. Cette langue présente plusieurs types de productions non modales impliquant une constriction glottale : [ʔ̞ , V̰ , Vʔ̞ , ʔ̞V, Vˀ, ʔV, ʔ] et formant des séquences telles que (C)VʔV, (C)V̰ CV, (C)Vʔ̞ CV ou (C)VʔCV, indépendamment de la hauteur tonale. De plus, ces productions présentent des variations de réalisation intra- et interlocuteur. Ce document présente l’ensemble des productions glottales relevées dans deux variétés du korebaju (korebaju et tama), sur la base de données audio et électroglottographiques, et recueillies sur le terrain entre 2022 et 2023 auprès de locuteurs natifs. Nous souhaitons présenter l’inventaire et la distribution des différents types de productions glottales en proposant leurs caractérisations acoustiques compte tenu des variations intra- et interlocuteurs pour ainsi ouvrir une discussion avec des experts en traitement automatique de la parole en vue du développement d’un outil de segmentation et labélisation (semi) automatique des corpus korebaju.},
  keywords = {Glottalisation,koreguaje,langues tukano,phonétique,phonologie},
  file = {C:\Users\Yu\Zotero\storage\75C242UF\Vega Rodriguez 和 Vallée - 2023 - Segmentation et analyse de productions non modales.pdf}
}

@inproceedings{rodriguezVowelSystemKorebaju2019,
  title = {The {{Vowel System}} of {{Korebaju}}},
  booktitle = {Interspeech 2019},
  author = {Rodriguez, Jenifer Vega},
  date = {2019-09-15},
  pages = {3975--3979},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2019-3210},
  url = {https://www.isca-archive.org/interspeech_2019/rodriguez19_interspeech.html},
  urldate = {2024-05-10},
  eventtitle = {Interspeech 2019},
  langid = {english}
}

@article{tranAutomaticSpeechRecognition2024,
  title = {Automatic {{Speech Recognition}} of {{Vietnamese}} for a {{New Large-Scale Corpus}}},
  author = {Tran, Linh Thi Thuc and Kim, Han-Gyu and La, Hoang Minh and Van Pham, Su},
  date = {2024-01},
  journaltitle = {Electronics},
  volume = {13},
  number = {5},
  pages = {977},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-9292},
  doi = {10.3390/electronics13050977},
  url = {https://www.mdpi.com/2079-9292/13/5/977},
  urldate = {2024-06-04},
  abstract = {Vietnamese is an under-resourced language. The requirement for a large-scale and high-quality Vietnamese speech corpus increases on demand. We introduce a new large-scale Vietnamese speech corpus with 100.5 h collected from various audio sources in the Internet. The raw collected audio was processed to obtain clean speech. Transcription of the clean speech was made manually. The new corpus was analyzed in terms of gender, topic and regional dialect. Results shows that the new corpus has good diversity of genders, topics and regional dialects. We also evaluated the new corpus using state-of-the-art automatic speech recognition models like LAS and Speech-Transformer for multiple scenarios. This is the first time that these models have been applied to Vietnamese speech recognition and obtained reasonable results. Simulation results showed that the new corpus would be a good dataset for the Vietnamese ASR tasks because it reflected correctly difficulties in recognizing speech from different dialects and topic domains.},
  issue = {5},
  langid = {english},
  keywords = {automatic speech recognition,LAS,SpecAugment,transformer,Vietnamese corpora},
  file = {C:\Users\Yu\Zotero\storage\X6XZM3VX\Tran et al. - 2024 - Automatic Speech Recognition of Vietnamese for a N.pdf}
}

@online{Trello2024,
  title = {Trello},
  date = {2024},
  url = {https://trello.com/fr},
  urldate = {2024-06-05},
  file = {C:\Users\Yu\Zotero\storage\ABKAF47S\fr.html}
}

@book{tubachParoleSonTraitement1989,
  title = {La Parole et son traitement automatique},
  author = {Tubach, Jean-Pierre},
  with = {{Calliope}},
  date = {1989},
  series = {Collection technique et scientifique des Télécommunications},
  publisher = {Masson},
  location = {Paris Milan Barcelone},
  isbn = {978-2-225-81516-4},
  langid = {fre}
}

@online{unlikelyaiFinetuningDeploymentCustom2024,
  title = {Fine-Tuning and {{Deployment}} of {{Custom ASR Models}}},
  author = {{Unlikely AI}},
  date = {2024-03-28T15:26:14},
  url = {https://medium.com/@unlikely_ai/fine-tuning-and-deployment-of-custom-asr-models-9e0eb5c45496},
  urldate = {2024-06-24},
  abstract = {Introduction},
  langid = {english},
  organization = {Medium},
  file = {C:\Users\Yu\Zotero\storage\TNNPH8L5\fine-tuning-and-deployment-of-custom-asr-models-9e0eb5c45496.html}
}

@book{vasquezHierarchicalNeuralNetwork2013,
  title = {Hierarchical {{Neural Network Structures}} for {{Phoneme Recognition}}},
  author = {Vasquez, Daniel and Gruhn, Rainer and Minker, Wolfgang},
  date = {2013},
  series = {Signals and {{Communication Technology}}},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-34425-1},
  url = {https://link.springer.com/10.1007/978-3-642-34425-1},
  urldate = {2024-06-04},
  isbn = {978-3-642-34424-4 978-3-642-34425-1},
  langid = {english},
  file = {C:\Users\Yu\Zotero\storage\TZECYIF7\Vasquez et al. - 2013 - Hierarchical Neural Network Structures for Phoneme.pdf}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and family=Kaiser, given=Ł, prefix=ukasz, useprefix=false and Polosukhin, Illia},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2024-06-12},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  file = {C:\Users\Yu\Zotero\storage\SMV3E9SI\Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@online{vonplatenFineTuneMMSAdapter2023,
  title = {Fine-{{Tune MMS Adapter Models}} for Low-Resource {{ASR}}},
  author = {family=Platen, given=Patrick, prefix=von, useprefix=true},
  date = {2023},
  url = {https://huggingface.co/blog/mms_adapters},
  urldate = {2024-09-04},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {C:\Users\Yu\Zotero\storage\X8F9YMG5\mms_adapters.html}
}

@thesis{wangJiYuShenDuXueXiDeDuanDaoDuanCangYuYuYinShiBieYanJiu2023,
  type = {mathesis},
  title = {基於深度學習的端到端藏語語音識別研究},
  author = {Wang, Chao},
  date = {2023},
  institution = {Tibet University},
  location = {Lhassa},
  langid = {chinese},
  file = {C:\Users\Yu\Zotero\storage\6AKURI2F\基于深度学习的端到端藏语语音识别研究_王超.pdf}
}

@article{wangOverviewEndtoEndAutomatic2019,
  title = {An {{Overview}} of {{End-to-End Automatic Speech Recognition}}},
  author = {Wang, Dong and Wang, Xiaodong and Lv, Shaohe},
  date = {2019-08-07},
  journaltitle = {Symmetry},
  shortjournal = {Symmetry},
  volume = {11},
  number = {8},
  pages = {1018},
  issn = {2073-8994},
  doi = {10.3390/sym11081018},
  url = {https://www.mdpi.com/2073-8994/11/8/1018},
  urldate = {2024-06-12},
  abstract = {Automatic speech recognition, especially large vocabulary continuous speech recognition, is an important issue in the field of machine learning. For a long time, the hidden Markov model (HMM)-Gaussian mixed model (GMM) has been the mainstream speech recognition framework. But recently, HMM-deep neural network (DNN) model and the end-to-end model using deep learning has achieved performance beyond HMM-GMM. Both using deep learning techniques,},
  langid = {english},
  file = {C:\Users\Yu\Zotero\storage\GFSTVL52\Wang et al. - 2019 - An Overview of End-to-End Automatic Speech Recogni.pdf}
}

@online{Wav2vec,
  title = {Wav2vec},
  url = {https://github.com/facebookresearch/fairseq},
  urldate = {2024-06-14},
  file = {C:\Users\Yu\Zotero\storage\XBN6XNK7\fairseq.html}
}

@software{Whisper2024,
  title = {Whisper},
  date = {2024-06-14T15:47:51Z},
  origdate = {2022-09-16T20:02:54Z},
  url = {https://github.com/openai/whisper},
  urldate = {2024-06-14},
  abstract = {Robust Speech Recognition via Large-Scale Weak Supervision},
  organization = {OpenAI}
}

@thesis{wuMandarinSyllableRecognition2014,
  type = {mathesis},
  title = {Mandarin syllable recognition system based on ASAT frame},
  author = {Wu, Xing},
  date = {2014},
  institution = {{Beijing University of Posts and Telecommunications}},
  location = {Beijing},
  langid = {chinese},
  file = {C:\Users\Yu\Zotero\storage\WQM6UECD\基于ASAT框架的汉语音节识别系统_吴兴.pdf}
}

@online{xuSimpleEffectiveZeroshot2021,
  title = {Simple and {{Effective Zero-shot Cross-lingual Phoneme Recognition}}},
  author = {Xu, Qiantong and Baevski, Alexei and Auli, Michael},
  date = {2021-09-23},
  eprint = {2109.11680},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2109.11680},
  urldate = {2024-06-05},
  abstract = {Recent progress in self-training, self-supervised pretraining and unsupervised learning enabled well performing speech recognition systems without any labeled data. However, in many cases there is labeled data available for related languages which is not utilized by these methods. This paper extends previous work on zero-shot cross-lingual transfer learning by fine-tuning a multilingually pretrained wav2vec 2.0 model to transcribe unseen languages. This is done by mapping phonemes of the training languages to the target language using articulatory features. Experiments show that this simple method significantly outperforms prior work which introduced task-specific architectures and used only part of a monolingually pretrained model.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound},
  file = {C:\Users\Yu\Zotero\storage\WKH8YQR5\Xu et al. - 2021 - Simple and Effective Zero-shot Cross-lingual Phone.pdf}
}

@article{yeroyanEnablingASRLowResource,
  title = {Enabling {{ASR}} for {{Low-Resource Languages}}: {{A Comprehensive Dataset Creation Approach}}},
  author = {Yeroyan, Ara and Karpov, Nikolay},
  abstract = {In recent years, automatic speech recognition (ASR) systems have significantly improved, especially in languages with a vast amount of transcribed speech data. However, ASR systems tend to perform poorly for low-resource languages with fewer resources, such as minority and regional languages. This study introduces a novel pipeline designed to generate ASR training datasets from audiobooks, which typically feature a single transcript associated with hours-long audios. The common structure of these audiobooks poses a unique challenge due to the extensive length of audio segments, whereas optimal ASR training requires segments ranging from 3 to 15 seconds. To address this, we propose a method for effectively aligning audio with its corresponding text and segmenting it into lengths suitable for ASR training. Our approach simplifies data preparation for ASR systems in low-resource languages and demonstrates its application through a case study involving the Armenian language. Our method, which is ”portable” to many lowresource languages, not only mitigates the issue of data scarcity but also enhances the performance of ASR models for underrepresented languages.},
  langid = {english},
  file = {C:\Users\Yu\Zotero\storage\JGEF5TS9\Yeroyan and Karpov - Enabling ASR for Low-Resource Languages A Compreh.pdf}
}

@article{yuAcousticModelingBased2020,
  title = {Acoustic {{Modeling Based}} on {{Deep Learning}} for {{Low-Resource Speech Recognition}}: {{An Overview}}},
  shorttitle = {Acoustic {{Modeling Based}} on {{Deep Learning}} for {{Low-Resource Speech Recognition}}},
  author = {Yu, Chongchong and Kang, Meng and Chen, Yunbing and Wu, Jiajia and Zhao, Xia},
  date = {2020},
  journaltitle = {IEEE Access},
  volume = {8},
  pages = {163829--163843},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3020421},
  url = {https://ieeexplore.ieee.org/document/9180290},
  urldate = {2024-05-29},
  abstract = {The polarization of world languages is becoming more and more obvious. Many languages, mainly endangered languages, are of low-resource attribute due to lack of information. Both language conservation and cultural heritage face important challenges. Therefore, speech recognition for low- resource scenario has become a hot topic in the field of speech. Based on the complex network structures and huge model parameters, deep learning has become a powerful science in the process of speech recognition, which has a broad and far-reaching significance for the study of low-resource speech recognition. Aiming at the characteristic of low resource, this article reviews the history and research status of two kinds of acoustic models of deep learning neural networks and acoustic end-to-end structures. We further elaborate on several key techniques for improving performance in the two aspects of data and model training. There are two projects for low-resource languages introduced in this article. The possible future developments are finally pointed out. These works provide some reference for computer speech and language processing.},
  eventtitle = {{{IEEE Access}}},
  keywords = {acoustic model,Acoustics,automatic speech recognition,data augmentation,Feature extraction,Hidden Markov models,Logic gates,Low-resource languages,Machine learning,meta learning,multitask learning,Neural networks,Speech recognition,transfer learning},
  file = {C:\Users\Yu\Zotero\storage\I5C9KVCP\Yu 等 - 2020 - Acoustic Modeling Based on Deep Learning for Low-R.pdf}
}

@book{yuAutomaticSpeechRecognition2015,
  title = {Automatic {{Speech Recognition}}: {{A Deep Learning Approach}}},
  shorttitle = {Automatic {{Speech Recognition}}},
  author = {Yu, Dong and Deng, Li},
  date = {2015},
  series = {Signals and {{Communication Technology}}},
  publisher = {Springer London},
  location = {London},
  doi = {10.1007/978-1-4471-5779-3},
  url = {https://link.springer.com/10.1007/978-1-4471-5779-3},
  urldate = {2024-06-05},
  isbn = {978-1-4471-5778-6 978-1-4471-5779-3},
  langid = {english},
  file = {C:\Users\Yu\Zotero\storage\K5I6DZY6\Yu and Deng - 2015 - Automatic Speech Recognition A Deep Learning Appr.pdf}
}

@article{yuCrossLanguageEndtoEndSpeech2019,
  title = {Cross-{{Language End-to-End Speech Recognition Research Based}} on {{Transfer Learning}} for the {{Low-Resource Tujia Language}}},
  author = {Yu, Chongchong and Chen, Yunbing and Li, Yueqiao and Kang, Meng and Xu, Shixuan and Liu, Xueer},
  date = {2019-02},
  journaltitle = {Symmetry},
  volume = {11},
  number = {2},
  pages = {179},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2073-8994},
  doi = {10.3390/sym11020179},
  url = {https://www.mdpi.com/2073-8994/11/2/179},
  urldate = {2024-05-29},
  abstract = {To rescue and preserve an endangered language, this paper studied an end-to-end speech recognition model based on sample transfer learning for the low-resource Tujia language. From the perspective of the Tujia language international phonetic alphabet (IPA) label layer, using Chinese corpus as an extension of the Tujia language can effectively solve the problem of an insufficient corpus in the Tujia language, constructing a cross-language corpus and an IPA dictionary that is unified between the Chinese and Tujia languages. The convolutional neural network (CNN) and bi-directional long short-term memory (BiLSTM) network were used to extract the cross-language acoustic features and train shared hidden layer weights for the Tujia language and Chinese phonetic corpus. In addition, the automatic speech recognition function of the Tujia language was realized using the end-to-end method that consists of symmetric encoding and decoding. Furthermore, transfer learning was used to establish the model of the cross-language end-to-end Tujia language recognition system. The experimental results showed that the recognition error rate of the proposed model is 46.19\%, which is 2.11\% lower than the that of the model that only used the Tujia language data for training. Therefore, this approach is feasible and effective.},
  issue = {2},
  langid = {english},
  keywords = {cross-language end-to-end,low-resource speech recognition,transfer learning,Tujia language},
  file = {C:\Users\Yu\Zotero\storage\2JSQUKML\Yu 等 - 2019 - Cross-Language End-to-End Speech Recognition Resea.pdf}
}

@book{YuJieXiShenDuXueXiYuYinShiBieShiJian2016,
  title = {解析深度學習：語音識別實踐},
  author = {俞, 棟 and 鄧, 力},
  translator = {俞, 凱 and 錢, 彥旻},
  date = {2016},
  publisher = {電子工業出版社},
  location = {北京},
  isbn = {978-7-121-28796-1},
  langid = {chinese},
  annotation = {OCLC: 1099449747},
  file = {C:\Users\Yu\Zotero\storage\PLK3CFQ3\俞栋 and 邓力 - 2016 - 解析深度学习 语音识别实践.pdf}
}

@article{zellouLinguisticDisparitiesCrosslanguage2024,
  title = {Linguistic Disparities in Cross-Language Automatic Speech Recognition Transfer from {{Arabic}} to {{Tashlhiyt}}},
  author = {Zellou, Georgia and Lahrouchi, Mohamed},
  date = {2024-01-03},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {14},
  number = {1},
  pages = {313},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-50516-3},
  url = {https://www.nature.com/articles/s41598-023-50516-3},
  urldate = {2024-06-04},
  abstract = {Tashlhiyt is a low-resource language with respect to acoustic databases, language corpora, and speech technology tools, such as Automatic Speech Recognition (ASR) systems. This study investigates whether a method of cross-language re-use of ASR is viable for Tashlhiyt from an existing commercially-available system built for Arabic. The source and target language in this case have similar phonological inventories, but Tashlhiyt permits typologically rare phonological patterns, including vowelless words, while Arabic does not. We find systematic disparities in ASR transfer performance (measured as word error rate (WER) and Levenshtein distance) for Tashlhiyt across word forms and speaking style variation. Overall, performance was worse for casual speaking modes across the board. In clear speech, performance was lower for vowelless than for voweled words. These results highlight systematic speaking mode- and phonotactic-disparities in cross-language ASR transfer. They also indicate that linguistically-informed approaches to ASR re-use can provide more effective ways to adapt existing speech technology tools for low resource languages, especially when they contain typologically rare structures. The study also speaks to issues of linguistic disparities in ASR and speech technology more broadly. It can also contribute to understanding the extent to which machines are similar~to, or different from, humans in mapping the acoustic signal to discrete linguistic representations.},
  langid = {english},
  keywords = {Human behaviour,Psychology},
  file = {C:\Users\Yu\Zotero\storage\MG2U7YP9\Zellou and Lahrouchi - 2024 - Linguistic disparities in cross-language automatic.pdf}
}
